{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业者：胡戈行 - 2019年10月7日 11:46:03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已作为Lecture-01-01rules.py 和 Lecture-01-02probability.py，提交至 https://github.com/Gexingh/gexingh.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "\n",
    "回答以下问题，并将问题发送至 minchuian.gao@gmail.com 中：\n",
    "\n",
    "2019年10月9日 19:21:40\n",
    "\n",
    "    2.0 My background：\n",
    "    \n",
    "      胡戈行，1990年2月生，2019年29岁，广东佛山人。\n",
    "      中国海洋大学  港口航道与海岸工程   本科     2009年09月– 2013年07月   4年\n",
    "      中国海洋大学  水利工程          硕士     2013年09月– 2015年06月   2年\n",
    "      江门市水利局江新联围管理处       职工     2015年08月 –2019年03月   4年\n",
    "      \n",
    "     【高老师，像我这样29岁马上30岁，非计算机专业的情况，找工作的时候会受企业欢迎吗？在学习过程中需要注意什么，找工作的时候该注意什么，能找到年薪22.5w的工作吗？】\n",
    "      \n",
    "    2.1. what do you want to acquire in this course？\n",
    "      年薪22.5w+的入门工作一份，进而谋求大的平台和更好的机会和待遇。\n",
    "    \n",
    "    2.2. what problems do you want to solve？\n",
    "      1、我计划从技术入手，然后转产品经理或者运营销售，最终5到10年后期待能构建能自我盈利不菲的平台，培养自己10年20年甚至50年后也依旧会需要用到的技能。根据人们的需求寻找合适及常见的技术进行对接，将AI项目落地，服务好人们，让人们过上更加美好的生活，并在此过程中积累财富，与更优秀的人建立深厚的关系。\n",
    "      2、回顾历史：\n",
    "        1993年家庭计算机来临的时候，当时谁知道怎么赚钱；\n",
    "        2000年互联网来临的时候，当时谁知道怎么赚钱；\n",
    "        2009年移动互联网来临的时候，当时谁知道怎么赚钱；\n",
    "        2019年人工智能来临的时候，当时谁知道怎么赚钱；\n",
    "        放在当时谁都不知道怎么赚钱，都只能是说感觉自己是对的，然后去尝试，有些人猜对了，有些人猜不对，但在对的风口上，猜对的机率大很多。做好积累，及时总结，认真思考，用心留意机会。\n",
    "      \n",
    "      3、（1）人们需要什么？\n",
    "       A.曾经人们需要什么：\n",
    "         1990年 国内基本没有家庭计算机。\n",
    "         \n",
    "         1995年 小霸王游戏机，大量单机像素游戏。\n",
    "         \n",
    "         1997年 国内初步出现家庭计算机，此时的计算机仍以单机为主，主要是用来：\n",
    "           a.公司：处理公告文件，扫描存储文件，处理财务报表，记录数据。\n",
    "           b.家庭：听歌、打单机游戏、看VCD、DVD看电影，扫雷，excel，星际争霸。\n",
    "           \n",
    "         1999年 拨号上网，上网则以浏览文字信息网站为主，带宽有限，各种信息初步传播，电子商务系统起步发展。      \n",
    "           \n",
    "         2000年 互联网开始普及，门户新闻网站，电子贸易进一步发展。\n",
    "           \n",
    "         2005年 搜索引擎、BBS，聊天室，在线游戏，CS等，淘宝\n",
    "         \n",
    "         2007年 车载导航仪，后来被高德地图APP取代。\n",
    "           \n",
    "         2009年 团购，百团大战之后剩下美团、大众点评。\n",
    "         \n",
    "         2012年 微信，手机APP，迈入智能手机时代 \n",
    "           \n",
    "         小米也想着把国外的先进技术搬进国内，做国内智能手机，埋头做技术，但没想到被华为降维打击打了一记重拳，现在只能往下游朝智能家电走，智能家电也可以，但市场明显没那么大。\n",
    "         华为是怎么起来的，先做2B，积累第一桶金，然后每次迈半小步，解决切实需求，然后再持续研发投入，最后主攻自己业务之余顺手降维打击手机业务并建立知名度。\n",
    "         两者格局不一样。\n",
    "      \n",
    "         2012-2017 \n",
    "           1、做手机APP的人赚钱了，替各个公司做手机APP并进行维护。\n",
    "           2、做手游的人赚钱了。\n",
    "           3、做网页游戏的人赚钱了。\n",
    "           4、做社交卖广告的人赚钱了。\n",
    "           5、小米做手机也赚钱了。\n",
    "           6、那么问题来了，AI赚钱吗，AI怎么赚钱。\n",
    "         \n",
    "         B.现在人们需要什么：\n",
    "           ①相同的努力能换来更好的生活条件，有食物，有衣穿，有房住，有车开，能旅游，有人爱，有选择，有保障。\n",
    "           ②不同收入意味着什么：\n",
    "           （1）月入2000，没车没房没女友，夏天买一年买两件衣服，冬天两年买一件，很少下馆子，攒一年的钱给自己换nokia手机（1k），再攒一年给母上大人换手机。基本没存款，年轻心态好，不觉得穷是啥压力。生活是无忧无虑的蓝天白云。\n",
    "           （2）月入5000，开始相亲找女友，周末天天下馆子，基本上100左右的东西想买就买，超市里的吃的想吃啥吃啥。手机换成2k的安卓机，未来我一定要换超酷的苹果。存款几万。未来的金光大道在我脚下。\n",
    "           （3）月入15000，有底气买奢侈品了，给女友换名牌包，1000以内的东西基本上想买就买。父母帮忙下买房，存款全砸进去了，月月还贷。攒钱结婚，舍不得给自己花，手机还是2k的。生活的压力慢慢压在肩头。工作就是在星空下去上班，又在星空下下班回家。\n",
    "           （4）月入30000，买车了，对社会与工作中的种种不公平渐渐麻木，每天挣扎在各种人际关系与职场政治。认识的存款几百万的人群都没有什么安全感，还在挣扎，自己几十万的存款无法给自己带来什么安全感和幸福感。手机还是2k的安卓机。\n",
    "          总结下，收入的增多会让我们生活变得更便利，每一个层次都会随着收入的提高，免去了很多烦恼，但随即会给你同样意想不到的烦恼和痛苦。\n",
    "          \n",
    "          所以AI的目的之一应该是让大家相同的努力下，能获得更好的生活水平，享受更多的便利，给人以更多的幸福感。\n",
    "          \n",
    "        （2）目前AI能做什么，省去大量重复工作。\n",
    "          ①NLP方面：语音识别+决策树模型：客服服务、兼职秘书服务、微博舆论监测、文章提纲、语音翻译、司法笔录。\n",
    "          ②CV方面：兼职门卫、兼职收费员（ETC&各大停车场）、老师课堂学生认真程度监视、支付认证、ZAO影视换脸（美化、娱乐）、自动驾驶（便利、安全）。\n",
    "          ③推荐系统：抖音、淘宝推荐、股票基金推荐等。\n",
    "\n",
    "        （3）我能做什么\n",
    "          积累技术、人缘，掌握实战项目后，找机会将AI落地，解决人们的需求，并实现自我财富积累，了解社会发展规律，将财富投入到更能产生价值的领域中。\n",
    "      \n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "      1、扎实的大学理论基础（高数、英语、政治），有一定的学习能力。\n",
    "      2、97年开始接触计算机，对计算机历史发展规律有一定的认识。\n",
    "      3、身处改革开放前沿城市附近，能经常去深圳，去深圳需要克服的困难少一些。\n",
    "      4、在政府基层工作四年，对国家发展历史，社会情况有大概认识。\n",
    "    \n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "      1、非计算机专业，缺失一大片计算机校友同学人脉，十年二十年三十年后，可能还是比不过某些计算机专业出身的人，但还是挣的比水利行业多。\n",
    "      除非能向上再升一大大截，远高出计算机行业，那就可以部分忽略非计算机专业缺失人脉的问题了，但前中期这个缺失还是很难受的。\n",
    "      2、参加工作4年并没有在计算机行业打滚，也没有在广深一线城市混，缺少编程基本功，需要努力补全。\n",
    "      3、缺乏参加社会工作经验，在政府机关呆惯了，可能并不习惯社会工作，过渡时期需要有个老师傅指导一下。\n",
    "      4、草根出身，缺乏家族人脉资金支持，需要靠自己去摸索出社会规律，在某些同等条件机会面前，拼不过有家底有资金的对手，但在风口上，机会总归还是会有的，需要更仔细的去思考寻找，能让自己落地的项目。    \n",
    "    \n",
    "    2.5. How will you plan to study in this course period?\n",
    "      每天7：30起床，8：30上班，17：30下班，18：30回到家，19：00-23：00 学习4小时，早上6：00-7：30学习1.5小时，周1到1学习4晚，周六学习1天（8：30-17：30 = 9h），周日学习1天（9+4h）。\n",
    "      初步一周计划学习时间（35小时）。\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至minchuian.gao@gmail.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.10.8日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1、机器翻译；2、拼写检查；3、金融报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1、Github，网上公用代码文件夹：①储存代码；②学习优秀代码；③评论区交流。\n",
    "2、Jupyter 方便编码演示；\n",
    "3、Pycharm 正规python编辑器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:$$ Pr(sentence) = Pr(w_1 \\cdot w_2 \\cdots w_n) = \\prod \\frac{count(w_i, w_{i+1})}{count(w_i)}$$\n",
    "概率模型，是指其中每一事件取决于前面的事件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:赌博；抽奖；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:1、当使用概率模型时，语法规则并不重要，它从统计学角度计算词语的出现概率。\n",
    "2、传统的语法语义模型需要定义特定的规则，当出现成千上万的对话时工作量非常巨大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:n-gram模型；\n",
    "此外还有神经网络语言模型(Neural language models)；word2vec模型；CBOW模型；Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:文本摘要；金融报告；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:一元语言模型。\n",
    "举一个实际的例子，比如有句子：见证大上海银行业间合作。我们有可能通过最长匹配方式拿到分词“上海银行”，事实上这个句子和\"上海银行\"没有关系。这个就是最长匹配方式导致的歧义问题。\n",
    "从后向前寻找每个切分词\n",
    "第1个是\"合作\"，这个词占了2个坑；\n",
    "所以第2个是序号9即\"间\"，这个词占了1个坑；\n",
    "所以第3个是序号8即\"银行业\"，这个词占了3个坑；\n",
    "所以第4个是序号5即\"上海\"，这个词占了2个坑；\n",
    "所以第5个是序号3即\"大\"，这个词占了1个坑；\n",
    "所以第6个是序号2即\"见证\"，这个词占了2个坑；\n",
    "最后第7个是序号0，不好意思没有这个坑了。\n",
    "通过以上计算我们得到了切词为：见证/大/上海/银行业/间/合作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:劣势：有些词出现的概率很高，但统计起来并没有意义；同时有可能漏掉某些关键词。\n",
    "优势：容易计算概率，模型简单。运算速度快，但准确性较低，只能用于粗略判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:2-gram models将旁边的语素也纳入了考虑范围，提高了准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor = '''\n",
    "\n",
    "doctor = 称呼 问候 询问 结束\n",
    "称呼 = 先生， | 女生， | 小朋友， \n",
    "问候 = 您好， | 你好，\n",
    "询问 = 请问您哪儿不舒服 | 请问您需要什么 | 请问您怎么了\n",
    "结束 = ？\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = '''\n",
    "\n",
    "patient = 称呼 问候 回答 结束\n",
    "称呼 = 医生， | 医师，  \n",
    "问候 = 您好， | 你好， \n",
    "回答 = 我感冒了 | 我喉咙不舒服 | 我最近感觉压力很大 | 我头疼 | 我最近老是失眠 | 我想生个二胎，需要注意什么\n",
    "结束 = 。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "医生： 女生，您好，请问您怎么了？\n",
      "患者： 医师，您好，我最近老是失眠。\n",
      "\n",
      "\n",
      "医生： 女生，您好，请问您哪儿不舒服？\n",
      "患者： 医师，你好，我感冒了。\n",
      "\n",
      "\n",
      "医生： 女生，您好，请问您哪儿不舒服？\n",
      "患者： 医师，你好，我喉咙不舒服。\n",
      "\n",
      "\n",
      "医生： 女生，你好，请问您哪儿不舒服？\n",
      "患者： 医师，您好，我头疼。\n",
      "\n",
      "\n",
      "医生： 女生，您好，请问您需要什么？\n",
      "患者： 医生，你好，我最近感觉压力很大。\n",
      "\n",
      "\n",
      "医生： 先生，你好，请问您怎么了？\n",
      "患者： 医师，您好，我头疼。\n",
      "\n",
      "\n",
      "医生： 小朋友，您好，请问您需要什么？\n",
      "患者： 医师，你好，我喉咙不舒服。\n",
      "\n",
      "\n",
      "医生： 小朋友，你好，请问您需要什么？\n",
      "患者： 医师，您好，我最近感觉压力很大。\n",
      "\n",
      "\n",
      "医生： 小朋友，你好，请问您怎么了？\n",
      "患者： 医生，你好，我最近老是失眠。\n",
      "\n",
      "\n",
      "医生： 先生，您好，请问您哪儿不舒服？\n",
      "患者： 医生，你好，我想生个二胎，需要注意什么。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def get_generation_by_gram(grammar_str: str, target, stmt_split='=',or_split='|'):\n",
    "    rules = dict()\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line: continue\n",
    "        stmt,expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = expr.split(or_split)\n",
    "    generated = generate(rules, target=target)\n",
    "    #print(rules)\n",
    "    return generated\n",
    "\n",
    "def generate(grammar_rule, target):\n",
    "    if target in grammar_rule: #names\n",
    "        candidates = grammar_rule[target]\n",
    "        candidate = random.choice(candidates)\n",
    "        return ''.join(generate(grammar_rule,target=c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target\n",
    "\n",
    "for n in range(10) :\n",
    "    print(\"医生：\",get_generation_by_gram(doctor, target='doctor', stmt_split='='))\n",
    "    print(\"患者：\",get_generation_by_gram(patient, target='patient', stmt_split='='))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决思路\n",
    "\n",
    "一、数据选择：保险行业问询对话集\n",
    "\n",
    "二、数据格式分析：\n",
    "\n",
    "0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ++$++ Is  Disability  Insurance  Required  By  Law?\n",
    "\n",
    "序号  ++$++ 类型  ++$++ 中文问题 ++$++ 英文问题\n",
    "\n",
    "每一项以\"\\n\"结束\n",
    "\n",
    "三、解决思路：\n",
    "（1）文本清洗，获得所有纯文本\n",
    "    文件中的将数据按\"序号\"\"类型\"\"中文问题\"\"英文问题\"归类\n",
    "（2）将这些文本进行切词\n",
    "    将中文问题进行切词，【并写入一个新的文件FILE2】\n",
    "（3）送入之前定义的语言模型中，判断文本的合理程度\n",
    "    随机抽取10项，送入FILE2，判断每一项中，中文问题的文本合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是关键人寿保险？ 6.856268842094055e-06\n",
      "\n",
      "\n",
      "如何取消地球人寿保险？ 4.955403895481013e-06\n",
      "什么汽车保险盖了鹿？ 3.997816469983672e-09\n",
      "医疗保险涵盖什么设备？ 5.1501378426685377e-08\n",
      "如何在田纳西州获得人寿保险牌照？ 1.0987962280014901e-10\n",
      "Humana提供人寿保险？ 4.286153376300378e-06\n",
      "关于租金每月多少保险？ 2.5041131810693105e-09\n",
      "混合年金如何安全？ 1.7897111003716786e-07\n",
      "为什么健康保险公司拒绝索赔？ 2.409044448567298e-09\n",
      "是不是没有健康保险？ 1.7495405441751303e-05\n",
      "哪个健康保险对我的家庭最好？ 1.1867308432975346e-12\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import random\n",
    "corpus = 'train.txt'\n",
    "FILE = open(corpus,encoding = 'UTF-8').read()\n",
    "FILE2 = open('temp_write.txt', 'w' , encoding = 'UTF-8')\n",
    "\n",
    "\n",
    "\n",
    "n_line = 0\n",
    "rules = dict()\n",
    "for line in FILE.split('\\n'):\n",
    "    if not line: continue\n",
    "    #0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ++$++ Is Disability Insurance Required By Law?\n",
    "    # 序号 类型 中文问题 英文问题\n",
    "    stmt,expr0,expr1,expr2 = line.split(' ++$++ ')\n",
    "    rules[stmt.strip()] = [expr0, expr1, expr2]\n",
    "    FILE2.write(expr1)\n",
    "    n_line = n_line + 1\n",
    "FILE2.close()\n",
    "FILE2 = open('temp_write.txt',encoding = 'UTF-8').read()\n",
    "#print(rules)\n",
    "\n",
    "def generate_by_pro(text_corpus, length=20):\n",
    "    return ''.join(random.sample(text_corpus,length))\n",
    "\n",
    "import jieba\n",
    "\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "TOKENS = cut(FILE2)\n",
    "from collections import Counter\n",
    "\n",
    "words_count = Counter(TOKENS)\n",
    "words_count.most_common(10)\n",
    "\n",
    "words_with_fre = [f for w,f in words_count.most_common()]\n",
    "import numpy as np\n",
    "\n",
    "_2_gram_words = [\n",
    "    TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)-1)\n",
    "]\n",
    "_2_gram_word_counts = Counter(_2_gram_words)\n",
    "\n",
    "def get_1_gram_count(word):\n",
    "    if word in words_count: return words_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1]\n",
    "def get_2_gram_count(word):\n",
    "    if word in _2_gram_word_counts: return _2_gram_word_counts[word]\n",
    "    else:\n",
    "        return _2_gram_word_counts.most_common()[-1][-1]\n",
    "def get_gram_count(word, wc):\n",
    "    if word in wc:return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]\n",
    "\n",
    "def two_gram_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    #print(tokens)\n",
    "    probability = 1\n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "\n",
    "        _two_gram_c = get_gram_count(word + next_word,_2_gram_word_counts)\n",
    "        _one_gram_c = get_gram_count(next_word, words_count)\n",
    "        pro = _two_gram_c / _one_gram_c\n",
    "\n",
    "        probability *= pro\n",
    "\n",
    "    return probability\n",
    "\n",
    "print('什么是关键人寿保险？',two_gram_model('什么是关键人寿保险？'))\n",
    "print('\\n')\n",
    "#6.856268842094055e-06\n",
    "\n",
    "for i in range(10) :\n",
    "    r = str(random.randint(0,n_line))\n",
    "    print(rules[r][1],two_gram_model(rules[r][1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思路：随机抽取10个题目2的中文问题，导入artical.txt语料库进行判断及排序，最后输出最优句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9298 医疗保险近期入院处罚多少？ 4.4551367726989214e-07\n",
      "9147 医疗保险补贴会因单次付款系统而消失？ 5.720453392724682e-10\n",
      "9312 人寿保险受益人意味着什么？ 6.286145335680161e-05\n",
      "11063 自雇人士可以扣除健康保险吗？ 1.5476936232716608e-12\n",
      "4164 健康保险如何影响经济？ 7.309467634002588e-10\n",
      "6599 残疾保险是否涵盖产假？ 2.9173996633320784e-06\n",
      "2792 我需要什么保险？ 1.6104724514192933e-07\n",
      "10580 什么是Obamacare去做医疗保险？ 3.866712391093078e-08\n",
      "2144 社会保障如何适应退休计划？ 5.245929158972637e-08\n",
      "176 财产保险与房主保险相同吗？ 2.9369207428617953e-09\n",
      "\n",
      "随机选择的10句中最合理句子为：\n",
      "9312 人寿保险受益人意味着什么？ 6.286145335680161e-05\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import random\n",
    "corpus = 'train.txt'\n",
    "FILE = open(corpus,encoding = 'UTF-8').read()\n",
    "n_line = 0\n",
    "rules = dict()\n",
    "for line in FILE.split('\\n'):\n",
    "    if not line: continue\n",
    "    #0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ++$++ Is Disability Insurance Required By Law?\n",
    "    # 序号 类型 中文问题 英文问题\n",
    "    stmt,expr0,expr1,expr2 = line.split(' ++$++ ')\n",
    "    rules[stmt.strip()] = [expr0, expr1, expr2]\n",
    "    n_line = n_line + 1\n",
    "\n",
    "corpus = 'article_9k.txt'\n",
    "FILE = open(corpus,encoding = 'UTF-8').read()\n",
    "\n",
    "import random\n",
    "def generate_by_pro(text_corpus, length=20):\n",
    "    return ''.join(random.sample(text_corpus,length))\n",
    "\n",
    "import jieba\n",
    "\n",
    "max_length = 1000000\n",
    "sub_file = FILE[:max_length]\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "TOKENS = cut(sub_file)\n",
    "from collections import Counter\n",
    "\n",
    "words_count = Counter(TOKENS)\n",
    "words_count.most_common(10)\n",
    "words_with_fre = [f for w,f in words_count.most_common()]\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "_2_gram_words = [\n",
    "    TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)-1)\n",
    "]\n",
    "_2_gram_word_counts = Counter(_2_gram_words)\n",
    "\n",
    "def get_1_gram_count(word):\n",
    "    if word in words_count: return words_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1]\n",
    "def get_2_gram_count(word):\n",
    "    if word in _2_gram_word_counts: return _2_gram_word_counts[word]\n",
    "    else:\n",
    "        return _2_gram_word_counts.most_common()[-1][-1]\n",
    "def get_gram_count(word, wc):\n",
    "    if word in wc:return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]\n",
    "\n",
    "def two_gram_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    probability = 1\n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "\n",
    "        _two_gram_c = get_gram_count(word + next_word,_2_gram_word_counts)\n",
    "        _one_gram_c = get_gram_count(next_word, words_count)\n",
    "        pro = _two_gram_c / _one_gram_c\n",
    "\n",
    "        probability *= pro\n",
    "\n",
    "    return probability\n",
    "\n",
    "\n",
    "\n",
    "def generate_best(): # you code here\n",
    "    ans = []\n",
    "    for i in range(10) :\n",
    "        r = str(random.randint(0,n_line))\n",
    "        ans.append((r,two_gram_model(rules[r][1])))\n",
    "        print(r,rules[r][1], two_gram_model(rules[r][1]))\n",
    "    ans2 = []\n",
    "    ans2 = sorted(ans , key=lambda x:x[1], reverse=True)\n",
    "    print(\"\\n随机选择的10句中最合理句子为：\")\n",
    "    print(ans2[0][0],rules[ans2[0][0]][1],ans2[0][1])\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "generate_best()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:（1）导入的是大众语料库，并不是针对保险医疗问题的语料库，可以提升语料库的专业性。\n",
    "（2）假如将所有语句进行排序，所需要的计算量较大，不过目前计算水平可以达到。\n",
    "（3）选择出最像人类的话，这件事似乎实际商业意义并不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过这个课程，我知道了：\n",
    "（1）随机函数random的运用，简单的生成对话，可以依据这个结合相关数据以及某些爬虫，生成一些预言类的鸡汤微信小程序以及星座预测等。\n",
    "    例如2020年来年运气预测，例如每日心语。\n",
    "（2）python基本语法\n",
    "（3）n-gram模型，如何从一个语料库导入语句，然后判断一个句子合理性；\n",
    "    ①切词 ②随机函数 ③two-gram模型\n",
    "（4）数据结构的一些基本用法\n",
    "（5）文件导入导出操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
